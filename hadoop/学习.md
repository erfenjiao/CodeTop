Hadoop Apache 开源分布式平台

以hdfs、MapReduce为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。

分布式文件系统 6.824



# mapreduce

**MapReduce is a programming model or pattern within the** [**Hadoop**](http://hadoop.apache.org/) **framework that is used to access big data stored in the Hadoop File System (HDFS).** It is a core component, integral to the functioning of the Hadoop framework.

MapReduce facilitates concurrent processing by splitting petabytes of data into smaller chunks, and processing them in parallel on Hadoop commodity servers. In the end, it aggregates all the data from multiple servers to return a consolidated output back to the application.

```
MapReduce 是Hadoop框架内的一种编程模型或模式，用于访问存储在 Hadoop 文件系统 (HDFS) 中的大数据。它是一个核心组件，是 Hadoop 框架功能的组成部分。

MapReduce 通过将数 PB 的数据拆分为更小的数据块并在 Hadoop 商用服务器上并行处理它们来促进并发处理。最后，它聚合来自多个服务器的所有数据，将合并后的输出返回给应用程序。
```



```
使用 MapReduce，不是将数据发送到应用程序或逻辑所在的位置，而是在数据已经驻留的服务器上执行逻辑，以加快处理速度。数据访问和存储是基于磁盘的——输入通常存储为包含结构化、半结构化或非结构化数据的文件，输出也存储在文件中。

MapReduce 曾经是唯一可以检索存储在 HDFS 中的数据的方法，但现在已不再如此。今天，还有其他基于查询的系统，例如 Hive 和 Pig，它们用于使用类似 SQL 的语句从 HDFS 检索数据。但是，这些通常与使用 MapReduce 模型编写的作业一起运行。那是因为 MapReduce 具有独特的优势。
```



## mapreduce如何工作

### Map函数

将来自磁盘的输入作为键值对处理，并声称另一组中间键值对作为输出。 map 是过滤和排序初始数据的强制性步骤

The input data is first split into smaller blocks. Each block is then assigned to a mapper for processing.

For example, if a file has 100 records to be processed, 100 mappers can run together to process one record each. Or maybe 50 mappers can run together to process two records each. The Hadoop framework decides how many mappers to use, based on the size of the data to be processed and the memory block available on each mapper server.

```
输入数据被分割成更小的块。将每个块分配给映射器进行处理
例如，如果一个文件由100个记录需要处理，则100个mapper一起处理，每个处理一条数据。或者也许 50 个映射器可以一起运行，每个映射器处理两个记录。Hadoop framework决定使用多少映射器，根据被处理数据集的大小和每个mapper server可用的内存块
```



### Reduce

After all the mappers complete processing, the framework shuffles and sorts the results before passing them on to the reducers. A reducer cannot start while a mapper is still in progress. All the map output values that have the same key are assigned to a single reducer, which then aggregates the values for that key.

```
在所有映射器完成处理后，框架在将结果传递给缩减器之前对结果进行混洗和排序。当 mapper 仍在进行中时，reducer 无法启动。
```



### Combine and Partition

There are two intermediate steps between Map and Reduce.

**Combine** is an optional process. The combiner is a reducer that runs individually on each mapper server. It reduces the data on each mapper further to a simplified form before passing it downstream.

This makes shuffling and sorting easier as there is less data to work with. Often, the combiner class is set to the reducer class itself, due to the cumulative and associative functions in the reduce function. However, if needed, the combiner can be a separate class as well.

**Partition** is the process that translates the <key, value> pairs resulting from mappers to another set of <key, value> pairs to feed into the reducer. It decides how the data has to be presented to the reducer and also assigns it to a particular reducer.

The default partitioner determines the hash value for the key, resulting from the mapper, and assigns a partition based on this hash value. There are as many partitions as there are reducers. So, once the partitioning is complete, the data from each partition is sent to a specific reducer.



## A MapReduce Example



### Map

For simplification, let's assume that the Hadoop framework runs just four mappers. Mapper 1, Mapper 2, Mapper 3, and Mapper 4.

The value input to the mapper is one record of the log file. The key could be a text string such as "file name + line number." The mapper, then, processes each record of the log file to produce key value pairs. Here, we will just use a filler for the value as '1.' The output from the mappers look like this:

Mapper 1 -> <Exception A, 1>, <Exception B, 1>, <Exception A, 1>, <Exception C, 1>, <Exception A, 1>
Mapper 2 -> <Exception B, 1>, <Exception B, 1>, <Exception A, 1>, <Exception A, 1>
Mapper 3 -> <Exception A, 1>, <Exception C, 1>, <Exception A, 1>, <Exception B, 1>, <Exception A, 1>
Mapper 4 -> <Exception B, 1>, <Exception C, 1>, <Exception C, 1>, <Exception A, 1>

Assuming that there is a combiner running on each mapper—Combiner 1 … Combiner 4—that calculates the count of each exception (which is the same function as the reducer), the input to Combiner 1 will be:

<Exception A, 1>, <Exception B, 1>, <Exception A, 1>, <Exception C, 1>, <Exception A, 1>











# Hadoop

The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.